{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO/7iy1RDALg/UnYUGHuQBh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Spring2024/blob/main/Corpus/TEDdata/will-be/Will_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data analysis: finding 'will' and 'be going to' patterns\n",
        "\n",
        "Written by MK316 (0606): Run the code and see how to get the result.\n",
        "\n",
        "+ You can get the result file in this page (also uploaded in your repository): will-begoingto.csv"
      ],
      "metadata": {
        "id": "PZoRFxpWGcLm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is-a2ddDDPeA"
      },
      "outputs": [],
      "source": [
        "# Data to read\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "datalink = \"https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/Cleantext0605F.csv\"\n",
        "data = pd.read_csv(datalink, encoding=\"utf-8\")\n",
        "data.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selected columns to analyze\n",
        "\n",
        "e.g., data => df: with selected columns (excluding the original text)\n",
        "\n",
        "+ Rename: Cleaned_length > Length, Cleanedtext01 > CText"
      ],
      "metadata": {
        "id": "O47gSBnfFZuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Select specific columns\n",
        "columns_to_select = ['TID','Cleaned_length','Wordcount','Cleanedtext01',]  # You can modify this list to select different columns\n",
        "df = data[columns_to_select]\n",
        "\n",
        "# Rename the columns\n",
        "df = df.rename(columns={'Cleaned_length': 'Length', 'Cleanedtext01': 'CText'})\n",
        "\n",
        "# Display the new DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "C46FqiePE4Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] Count 'will' and add a new column"
      ],
      "metadata": {
        "id": "4LO6cOwzGuDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to count complete matches of the word 'will'\n",
        "def count_exact_match(text, word):\n",
        "    # Use regex to find whole word matches only\n",
        "    return len(re.findall(r'\\b{}\\b'.format(re.escape(word)), text, re.IGNORECASE))\n",
        "\n",
        "# Apply the function to the 'CText' column\n",
        "df['Nwill'] = df['CText'].apply(lambda text: count_exact_match(text, 'will'))\n",
        "\n",
        "# Display the DataFrame to see the result\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "gC2BJywkHDjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Count 'be going to' and add a new column\n",
        "\n",
        "Explanation of the code:\n",
        "\n",
        "1. Data Preparation: We define a DataFrame df containing the provided example data. This includes texts that should have instances of the different forms of \"be going to\".\n",
        "2. Regular Expression Function: The function count_be_going_to utilizes the regular expression defined in pattern to match any form of \"am going to\", \"is going to\", \"are going to\", \"was going to\", and \"were going to\". The \\b ensures these are whole words (not part of larger words).\n",
        "3. Apply Function: We use df['CText'].apply(count_be_going_to) to apply our counting function to each row in the CText column. 4. The result is stored in a new column NbeGoingTo.\n",
        "Result Display: We print the updated DataFrame to verify our results.\n",
        "5. Regular Expression Pattern: The pattern now includes contractions like \"I'm\", \"he's\", \"she's\", \"it's\", \"they're\", \"you're\", and \"we're\" along with the original forms.\n",
        "6. Test Data: The example texts in the DataFrame have been updated to include these contraction forms to demonstrate that the updated function captures them correctly."
      ],
      "metadata": {
        "id": "D6zTrwBXHwd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a test version with a sample data (before we conduct our data)"
      ],
      "metadata": {
        "id": "zMYCogZiIPqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Create the DataFrame\n",
        "data = {\n",
        "    'TID': [1, 2],\n",
        "    'Length': [9008, 6663],\n",
        "    'Wordcount': [1657, 1223],\n",
        "    'CText': [\n",
        "        \"I'm going to explain the use of voice. We are going to see its effects. He's going to call yesterday.\",\n",
        "        \"They're going to participate in the event. She is going to travel tomorrow. You were going to tell me that story.\"\n",
        "    ]\n",
        "}\n",
        "df1 = pd.DataFrame(data)\n",
        "\n",
        "# Function to count occurrences of \"be going to\" in all forms including contractions\n",
        "def count_be_going_to(text):\n",
        "    # Pattern to match different forms of \"be going to\" including contractions\n",
        "    pattern = r\"\\b(?:am|is|are|was|were|I'm|he's|she's|it's|they're|you're|we're) going to\\b\"\n",
        "    # Find all matches with case insensitive search\n",
        "    return len(re.findall(pattern, text, re.IGNORECASE))\n",
        "\n",
        "# Apply the function to each row in 'CText'\n",
        "df1['NbeGoingTo'] = df1['CText'].apply(count_be_going_to)\n",
        "\n",
        "# Display the DataFrame to see the result\n",
        "print(df1)\n"
      ],
      "metadata": {
        "id": "WtOHpEhTHtfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Our data to process"
      ],
      "metadata": {
        "id": "x7MJSHZKIVeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to count occurrences of \"be going to\" in all forms including contractions\n",
        "def count_be_going_to(text):\n",
        "    # Pattern to match different forms of \"be going to\" including contractions\n",
        "    pattern = r\"\\b(?:am|is|are|was|were|I'm|he's|she's|it's|they're|you're|we're) going to\\b\"\n",
        "    # Find all matches with case insensitive search\n",
        "    return len(re.findall(pattern, text, re.IGNORECASE))\n",
        "\n",
        "# Apply the function to each row in 'CText'\n",
        "df['NbeGoingTo'] = df['CText'].apply(count_be_going_to)\n",
        "\n",
        "# Display the DataFrame to see the result\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "Fpp_oweuIXbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "BfEeEGlJIirK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spearman's rank correlation coefficient: Nwill and NbeGoingTo\n",
        "\n",
        "+ Spearman's rank correlation coefficient instead of Pearson's correlation coefficient. Spearman's correlation is more suitable for ordinal data or non-normally distributed variables, which is often the case with count data."
      ],
      "metadata": {
        "id": "rXEsM_73JRXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Calculate Spearman's rank correlation coefficient\n",
        "corr_wordcount_nwill = spearmanr(df['Wordcount'], df['Nwill'])\n",
        "corr_wordcount_nbegoingto = spearmanr(df['Wordcount'], df['NbeGoingTo'])\n",
        "corr_nwill_nbegoingto = spearmanr(df['Nwill'], df['NbeGoingTo'])\n",
        "\n",
        "# Print the results\n",
        "print(\"Correlation between Wordcount and Nwill:\", corr_wordcount_nwill)\n",
        "print(\"Correlation between Wordcount and NbeGoingTo:\", corr_wordcount_nbegoingto)\n",
        "print(\"Correlation between Nwill and NbeGoingTo:\", corr_nwill_nbegoingto)\n"
      ],
      "metadata": {
        "id": "4eeM5tGqJb6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation of Results\n",
        "\n",
        "Results:\n",
        "\n",
        "1) Correlation between Wordcount and Nwill: SignificanceResult(statistic=0.29014237616704847, pvalue=0.0034091910932080175)\n",
        "\n",
        "2) Correlation between Wordcount and NbeGoingTo: SignificanceResult(statistic=0.4254841848162721, pvalue=1.0196562679335068e-05)\n",
        "\n",
        "3) Correlation between Nwill and NbeGoingTo: SignificanceResult(statistic=0.21150465018464754, pvalue=0.0346508356304406)\n",
        "\n",
        "### 1. Correlation between Wordcount and Nwill:\n",
        "\n",
        "+ Coefficient (0.2901): This value suggests a weak positive correlation between the Wordcount and Nwill. This indicates that as the Wordcount increases, the count of the exact word \"will\" (Nwill) tends to increase slightly.\n",
        "+ P-value (0.0034): The p-value is less than 0.05, indicating that the correlation is statistically significant. This suggests that the observed correlation is unlikely to be due to random chance.\n",
        "\n",
        "### 2. Correlation between Wordcount and NbeGoingTo:\n",
        "\n",
        "+ Coefficient (0.4255): This value suggests a moderate positive correlation between the Wordcount and NbeGoingTo. As Wordcount increases, the instances of \"be going to\" phrases also tend to increase.\n",
        "+ P-value (about 0.00001): This extremely low p-value strongly suggests that the correlation is statistically significant and not due to random variation.\n",
        "\n",
        "### 3. Correlation between Nwill and NbeGoingTo:\n",
        "\n",
        "+ Coefficient (0.2115): This value indicates a weak positive correlation between the occurrences of \"will\" and the \"be going to\" phrases. While there is some level of association, it is relatively weak.\n",
        "+ P-value (0.0347): This p-value is just below the 0.05 threshold, suggesting that the correlation is statistically significant, although it's close to the boundary where we might consider it insignificant."
      ],
      "metadata": {
        "id": "N5hGoeHJKMw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalise the occurrences: Occurrences per 1000 words"
      ],
      "metadata": {
        "id": "-X2xx38dLYWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Normalize counts per 1000 words\n",
        "df['Nwill_per_1000'] = df['Nwill'] / df['Wordcount'] * 1000\n",
        "df['NbeGoingTo_per_1000'] = df['NbeGoingTo'] / df['Wordcount'] * 1000\n",
        "\n",
        "# Display the first few rows to verify the results\n",
        "print(df.head())\n",
        "\n",
        "# Calculate and print Spearman's rank correlation on normalized data\n",
        "from scipy.stats import spearmanr\n",
        "corr_normalized_nwill_nbegoingto = spearmanr(df['Nwill_per_1000'], df['NbeGoingTo_per_1000'])\n",
        "print(\"Correlation between normalized Nwill and NbeGoingTo:\", corr_normalized_nwill_nbegoingto)\n"
      ],
      "metadata": {
        "id": "gydWt3MkLecr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('will-begoingto.csv', index=False)"
      ],
      "metadata": {
        "id": "5Xu4roHxPVxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df is your DataFrame with the relevant data\n",
        "sns.scatterplot(x='Nwill', y='NbeGoingTo', data=df)\n",
        "plt.title('Scatter Plot of Nwill vs NbeGoingTo')\n",
        "plt.xlabel('Nwill')\n",
        "plt.ylabel('NbeGoingTo')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R37rQ3xmMYS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df[['Nwill', 'NbeGoingTo', 'Wordcount']])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p9hj_1NlMeBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Skewed Distributions: Both Nwill and NbeGoingTo are heavily skewed to the right, meaning that in most texts, these words/phrases appear infrequently. This could suggest specialized usage or contextual dependence of these terms.\n",
        "Weak Correlations: The weak correlations between Nwill and NbeGoingTo, and their respective correlations with Wordcount, suggest that the frequency of these terms is influenced by factors other than just the length of the text. This might include the style, genre, or specific thematic content of the texts."
      ],
      "metadata": {
        "id": "IAAfSKydN8wH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "To perform cluster analysis on your data to explore patterns in the occurrences of Nwill, NbeGoingTo, and their relationship with Wordcount, you can use the K-means clustering algorithm from the scikit-learn library. This method will help you identify groups (clusters) of texts that exhibit similar characteristics in terms of these variables.\n",
        "\n",
        "Here's how you can implement this in Python:\n",
        "\n",
        "Step-by-Step Guide for K-means Clustering\n",
        "Data Preparation: Ensure your data is suitable for clustering. You might consider normalizing the data due to different scales, especially if Wordcount is significantly higher in magnitude than Nwill and NbeGoingTo.\n",
        "\n",
        "Perform Clustering: Use the K-means algorithm to cluster the data.\n",
        "\n",
        "Visualize the Results: Create plots to visualize the clusters for better interpretation."
      ],
      "metadata": {
        "id": "l8RpESvPR2DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[['Wordcount', 'Nwill', 'NbeGoingTo']])\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)  # Choose the number of clusters\n",
        "df['cluster'] = kmeans.fit_predict(df_scaled)\n",
        "\n",
        "# Visualizing the clusters\n",
        "sns.pairplot(df, vars=['Wordcount', 'Nwill', 'NbeGoingTo'], hue='cluster', palette='viridis')\n",
        "plt.suptitle('Pair Plot of Text Data by Cluster', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Optionally, analyze cluster centers\n",
        "centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=['Wordcount', 'Nwill', 'NbeGoingTo'])\n",
        "print(\"Cluster centers (in original scale):\")\n",
        "print(centers)\n"
      ],
      "metadata": {
        "id": "igtCVeBLR3Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize the data\n",
        "# Even though the data is already normalized per 1000 words, standardizing can still help to equalize variance.\n",
        "scaler = StandardScaler()\n",
        "df_normalized = pd.DataFrame(scaler.fit_transform(df[['Nwill_per_1000', 'NbeGoingTo_per_1000']]),\n",
        "                             columns=['Nwill_per_1000', 'NbeGoingTo_per_1000'])\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)  # Choose the number of clusters based on domain knowledge or analysis\n",
        "df_normalized['cluster'] = kmeans.fit_predict(df_normalized)\n",
        "\n",
        "# Visualizing the clusters using normalized data\n",
        "sns.pairplot(df_normalized, vars=['Nwill_per_1000', 'NbeGoingTo_per_1000'], hue='cluster', palette='viridis')\n",
        "plt.suptitle('Pair Plot of Normalized Text Data by Cluster', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Optionally, print normalized cluster centers\n",
        "print(\"Normalized cluster centers:\")\n",
        "print(kmeans.cluster_centers_)\n"
      ],
      "metadata": {
        "id": "UctGLNutS9_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Interpreting Values:\n",
        "\n",
        "Positive Values: A positive z-score indicates that the feature's value is above the overall mean of that feature across all data points.\n",
        "Negative Values: A negative z-score indicates that the feature's value is below the overall mean."
      ],
      "metadata": {
        "id": "wI6pwmnMTxdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df[['Nwill_per_1000', 'NbeGoingTo_per_1000']]),\n",
        "                         columns=['Nwill_per_1000', 'NbeGoingTo_per_1000'])\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)  # Assume the optimal number of clusters is 3\n",
        "df['cluster'] = kmeans.fit_predict(df_scaled)\n",
        "\n",
        "# Grouping the TIDs by cluster\n",
        "cluster_groups = df.groupby('cluster')['TID'].apply(list)\n",
        "\n",
        "# Print the TIDs in each cluster\n",
        "print(\"Text IDs in each cluster:\")\n",
        "for cluster, tids in cluster_groups.items():\n",
        "    print(f\"Cluster {cluster}: Text IDs {tids}\")\n"
      ],
      "metadata": {
        "id": "c059XdXhUF9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Total text file (txt)\n"
      ],
      "metadata": {
        "id": "Ly2sMP6FDsDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Searching a match (complete or partial)\n",
        "\n",
        "# 3) Get user input for the word to find\n",
        "search_word = input(\"Enter the word to find: \")\n",
        "match_type = input(\"Type 'c' complete matches only, or 'p' for partial matches: \").lower()\n",
        "\n",
        "# 4) Function to find occurrences\n",
        "def find_occurrences(text, word, match_type):\n",
        "    occurrences = []\n",
        "    position = 0\n",
        "    while True:\n",
        "        if match_type == 'c':\n",
        "            # Find whole words only by using boundaries\n",
        "            position = text.lower().find(f' {word.lower()} ', position)\n",
        "        else:\n",
        "            position = text.lower().find(word.lower(), position)\n",
        "\n",
        "        if position == -1:  # No more occurrences found\n",
        "            break\n",
        "        # Calculate start and end positions for slicing\n",
        "        start = max(0, position - 30)\n",
        "        end = min(len(text), position + len(word) + 30)\n",
        "        occurrences.append(text[start:end])\n",
        "        position += len(word)  # Move past this occurrence\n",
        "\n",
        "    return occurrences\n",
        "\n",
        "occurrences = find_occurrences(combined_text, search_word, match_type)\n",
        "\n",
        "# 5) Decide how many occurrences to display\n",
        "print(f\"Total occurrences found: {len(occurrences)}\")\n",
        "print(\"=\"*50)\n",
        "if len(occurrences) > 10:\n",
        "    choice = input(\"More than 10 occurrences found. Type 'a' to display all or '10' to display only the first 10: \").lower()\n",
        "    print(\"=\"*50)\n",
        "    if choice == '10':\n",
        "        occurrences = occurrences[:10]\n",
        "\n",
        "# 6) Display occurrences\n",
        "for occurrence in occurrences:\n",
        "    print(occurrence)\n",
        "\n",
        "# 7) Print summary\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-wQ2UJGxDweZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}